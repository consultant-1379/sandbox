{"paragraphs":[{"text":"%md \n\n# Building a Movie Recommendation System\n\n## Overview\n\nThe purpose of this tutorial is to give an hands-on introduction to **Apache Spark** and its Machine Learning library **MLlib**, and show how to apply these technologies in the context of a real-world data problem.\n\nThe problem we are looking at is how to recommend movies to a user based on their own and other user's movie ratings.\n\nWe'll employ the following, typical science workflow:\n\n1. Download and assess data in a reproducible manner. We begin with a smaller data set.\n2. Data cleaning and exploratory analysis.\n3. Summarize the data. \n4. Build a predictive model and prediction performance.\n5. Predict your own movie ratings","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170116-165300_588663153","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Building a Movie Recommendation System</h1>\n<h2>Overview</h2>\n<p>The purpose of this tutorial is to give an hands-on introduction to <strong>Apache Spark</strong> and its Machine Learning library <strong>MLlib</strong>, and show how to apply these technologies in the context of a real-world data problem.</p>\n<p>The problem we are looking at is how to recommend movies to a user based on their own and other user's movie ratings.</p>\n<p>We'll employ the following, typical science workflow:</p>\n<ol>\n<li>Download and assess data in a reproducible manner. We begin with a smaller data set.</li>\n<li>Data cleaning and exploratory analysis.</li>\n<li>Summarize the data.</li>\n<li>Build a predictive model and prediction performance.</li>\n<li>Predict your own movie ratings</li>\n</ol>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4648"},{"text":"%md\n\n## Step 1: The data\n\nIn this tutorial, we'll be using the **MovieLens** data set which is collected, curated, and maintained by the GroupLens research group at the University of Minnesota.\n\nIt consists of non-commercial, public movie ratings contributed by MovieLens ([https://movielens.org/](https://movielens.org/)) users.\n\nMore detailed information and scientific publications can be accessed here: [https://grouplens.org/](https://grouplens.org/) \n\n### Downloading the data\n\nTo speed up things during the exploratory and model building phases, we'll initially use a smaller data set that contains 100004 ratings for 9125 movies. \n\nBelow is a small shell script that performs the download, unzips the downloaded file (for performance reasons), and eventually puts the data into **Hadoop's HDFS**. \n\nThe main reason for doing this is because in the real world, with a several orders of magnitudes larger data set, we will have no other option but to rely on distributed storage, such as HDFS.\n\nIt therefore makes sense to design our code with that condition in mind from the outset.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170119-115032_1935026800","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Step 1: The data</h2>\n<p>In this tutorial, we'll be using the <strong>MovieLens</strong> data set which is collected, curated, and maintained by the GroupLens research group at the University of Minnesota.</p>\n<p>It consists of non-commercial, public movie ratings contributed by MovieLens (<a href=\"https://movielens.org/\">https://movielens.org/</a>) users.</p>\n<p>More detailed information and scientific publications can be accessed here: <a href=\"https://grouplens.org/\">https://grouplens.org/</a></p>\n<h3>Downloading the data</h3>\n<p>To speed up things during the exploratory and model building phases, we'll initially use a smaller data set that contains 100004 ratings for 9125 movies.</p>\n<p>Below is a small shell script that performs the download, unzips the downloaded file (for performance reasons), and eventually puts the data into <strong>Hadoop's HDFS</strong>.</p>\n<p>The main reason for doing this is because in the real world, with a several orders of magnitudes larger data set, we will have no other option but to rely on distributed storage, such as HDFS.</p>\n<p>It therefore makes sense to design our code with that condition in mind from the outset.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4649"},{"text":"%sh\n\necho \"Downloading data to $(pwd)...\"\n\n# Let's set some variables.\nexport DATA_URL_PREFIX=\"http://files.grouplens.org/datasets/movielens/\"\nexport FLAVOUR=\"ml-latest-small\"\nexport DATA_URL=\"$DATA_URL_PREFIX/$FLAVOUR.zip\"\nexport HDFS_DATA_DIR=\"/user/data/$FLAVOUR\"\n\n# Download the data set from the MovieLens group site.\ncurl -sSL $DATA_URL -o $FLAVOUR.zip\nunzip -o $FLAVOUR.zip\n\n# Create a folder in HDFS and copy the data.\nsudo -u hdfs hadoop fs -mkdir -p $HDFS_DATA_DIR\nsudo -u hdfs hadoop fs -copyFromLocal -f $FLAVOUR/* $HDFS_DATA_DIR/\n\n\n# Clean up.\nrm -r $FLAVOUR\nrm $FLAVOUR.zip\n\necho -e \"\\n\\nDone\"","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20160905-173301_193364218","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4650","focus":true},{"text":"%md \n### Task 1: Hadoop CLI\n\nUse the `hadoop fs` command to list the data in the HDFS directory!\n\n* Read the hadoop fs command [documentation](https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/FileSystemShell.html#ls).\n* List the contents of `/user/data/ml-latest-small`.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170221-160926_1398101763","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Task 1: Hadoop CLI</h3>\n<p>Use the <code>hadoop fs</code> command to list the data in the HDFS directory!</p>\n<ul>\n<li>Read the hadoop fs command <a href=\"https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/FileSystemShell.html#ls\">documentation</a>.</li>\n<li>List the contents of <code>/user/data/ml-latest-small</code>.</li>\n</ul>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4651"},{"text":"%md\n\n### Task 1: Solution","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170116-212738_1368280800","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Task 1: Solution</h3>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4652"},{"text":"%sh\n\nhadoop fs -ls /user/data/ml-latest-small","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170116-210426_1190424134","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4653"},{"text":"%md\n\n## Step 2: Exploratory Analysis\n\nBefore applying more advanced techniques and writing code, let's first take a look at the data set, and do some basic exploration. This will give us a better sense of the data and what structure the data is in.\n\nFor doing this, we don't even have to write any code yet -- simple Unix tools are totally sufficient.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170116-215349_974644676","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Step 2: Exploratory Analysis</h2>\n<p>Before applying more advanced techniques and writing code, let's first take a look at the data set, and do some basic exploration. This will give us a better sense of the data and what structure the data is in.</p>\n<p>For doing this, we don't even have to write any code yet &ndash; simple Unix tools are totally sufficient.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4654"},{"text":"%sh\n\nhadoop fs -cat /user/data/ml-latest-small/movies.csv | head -n 5","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170120-105541_2008862764","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4655","focus":true},{"text":"%md \n\nBelow we print the number of samples in the movie CSV file.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170123-112957_1714968221","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Below we print the number of samples in the movie CSV file.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4656"},{"text":"%sh \n\nhadoop fs -cat /user/data/ml-latest-small/movies.csv | wc -l","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170123-113013_2116876592","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4657","focus":true},{"text":"%md\n\nThis confirms that we have data on 9125 movies. Note that the first row in the CSV file is the table header.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170123-113110_906126566","result":{"code":"SUCCESS","type":"HTML","msg":"<p>This confirms that we have data on 9125 movies. Note that the first row in the CSV file is the table header.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4658"},{"text":"%md \n### Task 2: List ratings CSV file\n\n* Use the Hadoop CLI with the `cat` command, as before, and pipe the contents to the UNIX `head` command.\n* Use `head` to print the **first five lines**.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170221-161925_1592166922","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Task 2: List ratings CSV file</h3>\n<ul>\n<li>Use the Hadoop CLI with the <code>cat</code> command, as before, and pipe the contents to the UNIX <code>head</code> command.</li>\n<li>Use <code>head</code> to print the <strong>first five lines</strong>.</li>\n</ul>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4659"},{"text":"%md\n\n### Task 2: Solution","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170221-162603_792891491","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Task 2: Solution</h3>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4660"},{"text":"%sh\n\n\nhadoop fs -cat /user/data/ml-latest-small/ratings.csv | head -n 5","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170120-105955_825654939","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4661"},{"text":"%md\n\n### Define a Data Schema in Spark\n\nAs can be seen from the outputs above, we are dealing with _structured_ data, as opposed to unstructured data. \n\nWe could try and use Spark's data schema **auto-discovery** feature, where Spark infers data types automatically. This works quite well for small and simple data sets that only use atomic types.\n\nHowever, auto-discovery also comes attached with a performance penalty, as Spark will need to parse the file completely, before creating a data frame. \n\nWhile the performance penalty is be negigible for the given (small) data set, data load performance will worsen with increasing size in the data set.\n\nTherefore it makes sense to use a pre-defined schema.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170120-110143_1345070342","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Define a Data Schema in Spark</h3>\n<p>As can be seen from the outputs above, we are dealing with <em>structured</em> data, as opposed to unstructured data.</p>\n<p>We could try and use Spark's data schema <strong>auto-discovery</strong> feature, where Spark infers data types automatically. This works quite well for small and simple data sets that only use atomic types.</p>\n<p>However, auto-discovery also comes attached with a performance penalty, as Spark will need to parse the file completely, before creating a data frame.</p>\n<p>While the performance penalty is be negigible for the given (small) data set, data load performance will worsen with increasing size in the data set.</p>\n<p>Therefore it makes sense to use a pre-defined schema.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4662"},{"text":"%pyspark\n\nfrom pyspark.sql.types import *\n\n# Define schemas for ratings and movie data.\nratings_df_schema = StructType(\n  [StructField('userId', IntegerType()),\n   StructField('movieId', IntegerType()),\n   StructField('rating', DoubleType())]\n)\nmovies_df_schema = StructType(\n  [StructField('ID', IntegerType()),\n   StructField('title', StringType())]\n)","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20160905-175347_976876351","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4663"},{"text":"%md \n\n### Spark's DataFrame / DataSet API\n\nThis now puts us in a position to leverage Spark 2.x's new **DataFrame** / **DataSet APIs**, which provides a set of higher-level data abstraction than what is offered by the more traditional _RDD's_. specifically for columnar data. In addition, using DataSets introduces strong typing.\n\n* You can read more about the actions and transformations available on these types here: [https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n\n* A neat comparison between DataSets and DataFrames can be found here: [https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/](https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/)\n\n\nSpark Data frames are really handy and from a Data Scientist's point of view because their APIs and usage is conecptually very close to similarly named data types in Pandas or R.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170120-110557_1632882441","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Spark's DataFrame / DataSet API</h3>\n<p>This now puts us in a position to leverage Spark 2.x's new <strong>DataFrame</strong> / <strong>DataSet APIs</strong>, which provides a set of higher-level data abstraction than what is offered by the more traditional <em>RDD's</em>. specifically for columnar data. In addition, using DataSets introduces strong typing.</p>\n<ul>\n<li><p>You can read more about the actions and transformations available on these types here: <a href=\"https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame</a></p>\n</li>\n<li><p>A neat comparison between DataSets and DataFrames can be found here: <a href=\"https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/\">https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/</a></p>\n</li>\n</ul>\n<p>Spark Data frames are really handy and from a Data Scientist's point of view because their APIs and usage is conecptually very close to similarly named data types in Pandas or R.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4664"},{"text":"%md\n\n### Loading the data into Spark from the CSV files\n\nNow we are ready to load the data into Spark for further processing.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170120-110939_1367410160","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Loading the data into Spark from the CSV files</h3>\n<p>Now we are ready to load the data into Spark for further processing.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4665"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import regexp_extract\nfrom pyspark.sql.types import *\n\n# Set the data set location.\nFLAVOUR = 'ml-latest-small'\n\nhdfs_dir = '/user/data/' + FLAVOUR\nratings_filename = hdfs_dir + '/ratings.csv'\nmovies_filename = hdfs_dir + '/movies.csv'\n\n\n# Read ratings.\nraw_ratings_df = spark.read.schema(ratings_df_schema).option(\"header\", True).csv(ratings_filename)\n\nratings_df = raw_ratings_df.drop('Timestamp')\n\n# Read movie data.\nraw_movies_df = spark.read.schema(movies_df_schema).option(\"header\", True).csv(movies_filename)\n\nmovies_df = raw_movies_df.drop('Genres').withColumnRenamed('movieId', 'ID')\n\n# Cache data.\nratings_df.cache()\nmovies_df.cache()\n\nassert ratings_df.is_cached\nassert movies_df.is_cached\n\nraw_ratings_count = raw_ratings_df.count()\nraw_movies_count = raw_movies_df.count()\n\n","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20160905-181742_1291090020","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4666"},{"text":"%md \n### Apache Spark: Operations on Data\n\nBroadly speaking, in Spark we can perform two different kinds of operations:\n\n* Actions\n* Transformations\n\nWe will now use an example of an action.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170221-163651_902138149","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Apache Spark: Operations on Data</h3>\n<p>Broadly speaking, in Spark we can perform two different kinds of operations:</p>\n<ul>\n<li>Actions</li>\n<li>Transformations</li>\n</ul>\n<p>We will now use an example of an action.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4667"},{"text":"%md\n\n### Task 3: How many movies and ratings do we have?\n\n* Above we have created 2 data frames:\n\n    `movies_df`\n    `ratings_df`\n    \n* We can use the [count()](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.GroupedData.count) function on the data frame to count the number of rows in it.\n* Print the results with Python's `print`.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170221-163307_1688603895","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Task 3: How many movies and ratings do we have?</h3>\n<ul>\n<li><p>Above we have created 2 data frames:</p>\n<p><code>movies_df</code>\n<br  /><code>ratings_df</code></p>\n</li>\n<li><p>We can use the <a href=\"https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.GroupedData.count\">count()</a> function on the data frame to count the number of rows in it.</p>\n</li>\n<li><p>Print the results with Python's <code>print</code>.</p>\n</li>\n</ul>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4668"},{"text":"%md \n\n### Task 3: Solution","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170221-164506_51176630","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Task 3: Solution</h3>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4669"},{"text":"%pyspark\n\nratings_count = ratings_df.count()\nmovies_count = movies_df.count()\n\n# Let's print a quick summary.\nprint 'There are %s ratings and %s movies in the datasets' % (ratings_count, movies_count)","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170221-164243_966672949","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4670"},{"text":"%md\n\nTo verify that everything has been loaded successfully, let's take a peek at the contents of the data frames.\n\nThe below example uses Zeppelin's built-in data display facility.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170120-111036_583711460","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To verify that everything has been loaded successfully, let's take a peek at the contents of the data frames.</p>\n<p>The below example uses Zeppelin's built-in data display facility.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4671"},{"text":"%pyspark\n\nz.show(movies_df.limit(10))\n","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"ID","index":0,"aggr":"sum"}],"values":[{"name":"title","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"ID","index":0,"aggr":"sum"},"yAxis":{"name":"title","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20170118-161922_153050418","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4672","focus":true},{"text":"%pyspark\n\nz.show(ratings_df.limit(10))","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709037_-181464597","id":"20160905-181811_1866959174","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4673","focus":true},{"text":"%md\n\n## Step 3: Summarizing the Data\n\nIn a similar fashion to the above, we can work our some simple summaries to better describe the data.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170119-115137_552973414","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Step 3: Summarizing the Data</h2>\n<p>In a similar fashion to the above, we can work our some simple summaries to better describe the data.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4674"},{"text":"%pyspark\n\nfrom pyspark.sql import functions as F\n\n# From ratingsDF, create a movie_ids_with_avg_ratings_df that combines the two DataFrames\nmovie_ids_with_avg_ratings_df = ratings_df.groupBy('movieId').agg(F.count(ratings_df.rating).alias(\"count\"), F.avg(ratings_df.rating).alias(\"average\"))\n\n# Note: movie_names_df is a temporary variable, used only to separate the steps necessary\n# to create the movie_names_with_avg_ratings_df DataFrame.\n\nmovie_names_df = movie_ids_with_avg_ratings_df.join(movies_df,  movie_ids_with_avg_ratings_df.movieId==movies_df.ID, 'inner').drop(movies_df.ID)\nmovie_names_with_avg_ratings_df = movie_names_df","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170117-195901_1783491843","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4675"},{"text":"%md\n\n### Movies with most votes\n\nWe can make use of `orderBy()` to return a Dataframe ordered by the number of votes recieved.\n\n* Documentation for [orderBy()](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy)","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170120-111106_194865753","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Movies with most votes</h3>\n<p>We can make use of <code>orderBy()</code> to return a Dataframe ordered by the number of votes recieved.</p>\n<ul>\n<li>Documentation for <a href=\"https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy\">orderBy()</a></li>\n</ul>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4676"},{"text":"%pyspark\n\nm_ordered = movie_names_with_avg_ratings_df.orderBy(\"count\", ascending = 0)\nz.show(m_ordered.limit(10))\n","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"movieId","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"movieId","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170117-201122_730391197","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4677","focus":true},{"text":"%md\n\n### Optional: Movies with least votes\n\nSimilarly, we can look at which movies have recieved the least votes.","dateUpdated":"2017-02-21T20:59:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170124-115615_214745554","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Optional: Movies with least votes</h3>\n<p>Similarly, we can look at which movies have recieved the least votes.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4678","dateFinished":"2017-02-21T20:59:22+0000","dateStarted":"2017-02-21T20:59:22+0000","focus":true},{"text":"%pyspark\n\nm_ordered_least = movie_names_with_avg_ratings_df.orderBy(\"count\", ascending = 1)\nz.show(m_ordered_least.limit(10))","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170124-115655_372093907","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4679","focus":true},{"text":"%md\n\n### Filter movies with at least 200 votes\n\nAs was shown in the previous section, there are some movies which only recieved 1 vote. For our recommendation system we should clearly give more weight to movies that recieved a reasonably large number of votes.\n\nSo next we are creating a data frame that consists of movies with at least 200 votes.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170120-115323_1772369193","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Filter movies with at least 200 votes</h3>\n<p>As was shown in the previous section, there are some movies which only recieved 1 vote. For our recommendation system we should clearly give more weight to movies that recieved a reasonably large number of votes.</p>\n<p>So next we are creating a data frame that consists of movies with at least 200 votes.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4680"},{"text":"%pyspark\n\n# Now get movies with more than 200 ratings.\nmovies_with_200_ratings_or_more = movie_names_with_avg_ratings_df.orderBy('average', ascending=False).filter('count >= 200')\nprint 'Movies with highest ratings:'\n\nz.show(movies_with_200_ratings_or_more.limit(10))","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"movieId","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"movieId","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170119-113857_1545682050","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4681","focus":true},{"text":"%md\n\n## Step 4: Building a predictive model for Movie Recommendations\n\nIn the most general sense, recommendation systems attempt to predict how a user might rate (in other words -- like) certain types of movies, music or consumer electronics as offered in online shops. \n\nThus, it can be used as a tool to increase sales by displpaying items that are most relevant to the user.\n\n\n### Collaborative Filtering\n\nNow we will proceed to implement a movie recommender using a technique called *Collaborative Filtering*. There are other techniques that may be used for the same purpose.\n\nThis Wikipedia article gives a really good overview of the method how it's used: [https://en.wikipedia.org/wiki/Collaborative_filtering](https://en.wikipedia.org/wiki/Collaborative_filtering).\n\n\n### Splitting data into training, test and validation sets\n\nAs is best practice, we use the following approach:\n\n* Build the model on the **training set** (60% of data).\n* Select best model using **validation set** (20% of data).\n* Assess model performance on held-out **test set** (20% of data).","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170117-205707_1066614680","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Step 4: Building a predictive model for Movie Recommendations</h2>\n<p>In the most general sense, recommendation systems attempt to predict how a user might rate (in other words &ndash; like) certain types of movies, music or consumer electronics as offered in online shops.</p>\n<p>Thus, it can be used as a tool to increase sales by displpaying items that are most relevant to the user.</p>\n<h3>Collaborative Filtering</h3>\n<p>Now we will proceed to implement a movie recommender using a technique called <em>Collaborative Filtering</em>. There are other techniques that may be used for the same purpose.</p>\n<p>This Wikipedia article gives a really good overview of the method how it's used: <a href=\"https://en.wikipedia.org/wiki/Collaborative_filtering\">https://en.wikipedia.org/wiki/Collaborative_filtering</a>.</p>\n<h3>Splitting data into training, test and validation sets</h3>\n<p>As is best practice, we use the following approach:</p>\n<ul>\n<li>Build the model on the <strong>training set</strong> (60% of data).</li>\n<li>Select best model using <strong>validation set</strong> (20% of data).</li>\n<li>Assess model performance on held-out <strong>test set</strong> (20% of data).</li>\n</ul>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4682"},{"text":"%pyspark\n\n# For reasons of reproducibility, set a seed.\nseed = 1222121193L\n\n\n(split_60_df, split_a_20_df, split_b_20_df) = ratings_df.randomSplit([0.6, 0.2, 0.2], seed)\n\ntraining_df = split_60_df.cache()\nvalidation_df = split_a_20_df.cache()\ntest_df = split_b_20_df.cache()\n\nprint('Size of split sets: Training set: {0}, validation set: {1}, test set: {2}\\n'.format(\n  training_df.count(), validation_df.count(), test_df.count())\n)\n\n\nz.show(training_df.limit(3))\n","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170119-114222_41843633","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4683","focus":true},{"text":"%md\n\n### Task 4: Display validation data ordered by descending rating\n\n* Through Zeppelin's `z.show()` display 5 rows of the validation set. \n* For this purpose, use the `limit(n)` function.\n* Make sure the displayed rows are **ordered by rating** in descending rating.\n* Documentation for [orderBy()](https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy)","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170221-171537_449976077","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Task 4: Display validation data ordered by descending rating</h3>\n<ul>\n<li>Through Zeppelin's <code>z.show()</code> display 5 rows of the validation set.</li>\n<li>For this purpose, use the <code>limit(n)</code> function.</li>\n<li>Make sure the displayed rows are <strong>ordered by rating</strong> in descending rating.</li>\n<li>Documentation for <a href=\"https://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy\">orderBy()</a></li>\n</ul>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4684"},{"text":"%md\n\n### Task 4: Solution","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709038_-180310350","id":"20170221-172002_1118335996","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Task 4: Solution</h3>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4685"},{"text":"%pyspark\n\nz.show(validation_df.orderBy('rating', ascending = False).limit(5))\n","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-141823_704345449","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4686"},{"text":"%pyspark\n\nz.show(test_df.limit(3))","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"userId","index":0,"aggr":"sum"}],"values":[{"name":"movieId","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"userId","index":0,"aggr":"sum"},"yAxis":{"name":"movieId","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-141834_2024487736","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4687","focus":true},{"text":"%md\n\n### Alternating Least Squares in Spark's MLlib\n\nNot going into much detail with this, but here are a few introductory pointers.\n\n<hr />\n![](https://raw.githubusercontent.com/bwv988/cf-tutorial/master/images/slide1.png)\n\n<hr />\n![](https://raw.githubusercontent.com/bwv988/cf-tutorial/master/images/slide2.png)\n\n<hr />\n![](https://raw.githubusercontent.com/bwv988/cf-tutorial/master/images/slide3.png)\n","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-114300_1737921264","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Alternating Least Squares in Spark's MLlib</h3>\n<p>Not going into much detail with this, but here are a few introductory pointers.</p>\n<hr />\n<p><img src=\"https://raw.githubusercontent.com/bwv988/cf-tutorial/master/images/slide1.png\" alt=\"\" /></p>\n<hr />\n<p><img src=\"https://raw.githubusercontent.com/bwv988/cf-tutorial/master/images/slide2.png\" alt=\"\" /></p>\n<hr />\n<p><img src=\"https://raw.githubusercontent.com/bwv988/cf-tutorial/master/images/slide3.png\" alt=\"\" /></p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4688"},{"text":"%md\n\n### Use Spark's MLlib to build ALS models\n\nRead more [here](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html).","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170221-195502_1237501659","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Use Spark's MLlib to build ALS models</h3>\n<p>Read more <a href=\"http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html\">here</a>.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4689"},{"text":"%pyspark\n\nfrom pyspark.ml.recommendation import ALS\n\n# Let's initialize our ALS learner\nals = ALS()\n\n# Now we set the parameters for the method\nals.setMaxIter(5)\\\n   .setSeed(seed)\\\n   .setRegParam(0.1)\\\n   .setUserCol('userId')\\\n   .setItemCol('movieId')\\\n   .setRatingCol('rating')\n\n# Now let's compute an evaluation metric for our test dataset\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Create an RMSE evaluator using the label and predicted columns\nreg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n\ntolerance = 0.03\nranks = [4, 8, 12]\nerrors = [0, 0, 0]\nmodels = [0, 0, 0]\nerr = 0\nmin_error = float('inf')\nbest_rank = -1\nfor rank in ranks:\n  # Set the rank here:\n  als.setRank(rank)\n  # Create the model with these parameters.\n  model = als.fit(training_df)\n  # Run the model to create a prediction. Predict against the validation_df.\n  predict_df = model.transform(validation_df)\n\n  # Remove NaN values from prediction (due to SPARK-14489)\n  predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n  # Run the previously created RMSE evaluator, reg_eval, on the predicted_ratings_df DataFrame\n  error = reg_eval.evaluate(predicted_ratings_df)\n  errors[err] = error\n  models[err] = model\n  print 'For rank %s the RMSE is %s' % (rank, error)\n  if error < min_error:\n    min_error = error\n    best_rank = err\n  err += 1\n\nals.setRank(ranks[best_rank])\nprint 'The best model was trained with rank %s' % ranks[best_rank]\nmy_model = models[best_rank]","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-114357_239745215","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4690","focus":true},{"text":"%md\n\n### Model assessment and prediction on the test data\n\nWe want to use the model that minimizes the RMSE (=best performance).\n\nThe below predictions are pretty close to the true values!","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-115316_1033013081","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Model assessment and prediction on the test data</h3>\n<p>We want to use the model that minimizes the RMSE (=best performance).</p>\n<p>The below predictions are pretty close to the true values!</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4691"},{"text":"%pyspark\n\n# In ML Pipelines, this next step has a bug that produces unwanted NaN values. We\n# have to filter them out. See https://issues.apache.org/jira/browse/SPARK-14489\npredict_df = my_model.transform(test_df)\n\n# Remove NaN values from prediction (due to SPARK-14489)\npredicted_test_df = predict_df.filter(predict_df.prediction != float('nan'))\n\n# Run the previously created RMSE evaluator, reg_eval, on the predicted_test_df DataFrame\ntest_RMSE = reg_eval.evaluate(predicted_test_df)\n\nprint('The model had a RMSE on the test set of {0}'.format(test_RMSE))\n\npredicted_test_df.show(10)","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-114408_1212275616","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4692","focus":true},{"text":"%md\n\n### Optional: Comparing models","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-114607_2044766350","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Optional: Comparing models</h3>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4693"},{"text":"%pyspark\n\navg_rating_df = training_df.groupBy().mean('rating')\nprint 'Average training set movie ratings:'\navg_rating_df.show(3, truncate=False)\n\n# Extract the average rating value. (This is row 0, column 0.)\ntraining_avg_rating = avg_rating_df.collect()[0][0]\n\nprint('The average rating for movies in the training set is {0}'.format(training_avg_rating))\n\n# Add a column with the average rating\ntest_for_avg_df = test_df.withColumn('prediction', F.lit(training_avg_rating))\ntest_for_avg_df.show(3, truncate=False)\n\n# Run the previously created RMSE evaluator, reg_eval, on the test_for_avg_df DataFrame\ntest_avg_RMSE = reg_eval.evaluate(test_for_avg_df)\n\nprint(\"The RMSE on the average set is {0}\".format(test_avg_RMSE))","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-115824_404059800","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4694","focus":true},{"text":"%md\n\n## Step 5: Predict yourself\n\nLet's use the model created to predict how we would rate. We will do the following.\n\n* Create a data frame with our own ratings for a couple of movies.\n* Add it to the training data set.\n* Build a predictive model, as before.\n* Assess performance.\n* Use this model to predict ratings on un-rated movies.\n\nBelow is a list of movies with the highest average rating. Select some movies from this and take a note of the `movieID`.","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-115845_895248477","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Step 5: Predict yourself</h2>\n<p>Let's use the model created to predict how we would rate. We will do the following.</p>\n<ul>\n<li>Create a data frame with our own ratings for a couple of movies.</li>\n<li>Add it to the training data set.</li>\n<li>Build a predictive model, as before.</li>\n<li>Assess performance.</li>\n<li>Use this model to predict ratings on un-rated movies.</li>\n</ul>\n<p>Below is a list of movies with the highest average rating. Select some movies from this and take a note of the <code>movieID</code>.</p>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4695"},{"text":"%pyspark\n\nprint 'Most rated movies:'\nprint '(average rating, movie name, number of reviews, movie ID)'\n\ntmp = movies_with_200_ratings_or_more.orderBy(movies_with_200_ratings_or_more['average'].desc())\n\nz.show(tmp)\n","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"movieId","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"movieId","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-115934_2039562721","result":{"code":"SUCCESS","type":"TABLE","msg":"movieId\tcount\taverage\ttitle\n858\t200\t4.4875\tGodfather, The (1972)\n318\t311\t4.487138263665595\tShawshank Redemption, The (1994)\n50\t201\t4.370646766169155\tUsual Suspects, The (1995)\n527\t244\t4.30327868852459\tSchindler's List (1993)\n608\t224\t4.256696428571429\tFargo (1996)\n296\t324\t4.256172839506172\tPulp Fiction (1994)\n2858\t220\t4.236363636363636\tAmerican Beauty (1999)\n1196\t234\t4.232905982905983\tStar Wars: Episode V - The Empire Strikes Back (1980)\n260\t291\t4.221649484536083\tStar Wars: Episode IV - A New Hope (1977)\n1198\t220\t4.193181818181818\tRaiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)\n2571\t259\t4.183397683397684\tMatrix, The (1999)\n4993\t200\t4.1825\tLord of the Rings: The Fellowship of the Ring, The (2001)\n2959\t202\t4.178217821782178\tFight Club (1999)\n593\t304\t4.1381578947368425\tSilence of the Lambs, The (1991)\n1210\t217\t4.059907834101383\tStar Wars: Episode VI - Return of the Jedi (1983)\n356\t341\t4.05425219941349\tForrest Gump (1994)\n47\t201\t4.034825870646766\tSeven (a.k.a. Se7en) (1995)\n1270\t226\t4.015486725663717\tBack to the Future (1985)\n589\t237\t4.006329113924051\tTerminator 2: Judgment Day (1991)\n457\t213\t3.9530516431924885\tFugitive, The (1993)\n110\t228\t3.9451754385964914\tBraveheart (1995)\n150\t200\t3.9025\tApollo 13 (1995)\n1\t247\t3.8724696356275303\tToy Story (1995)\n364\t200\t3.7775\tLion King, The (1994)\n590\t202\t3.717821782178218\tDances with Wolves (1990)\n480\t274\t3.7062043795620436\tJurassic Park (1993)\n588\t215\t3.6744186046511627\tAladdin (1992)\n780\t218\t3.4839449541284404\tIndependence Day (a.k.a. ID4) (1996)\n\n","comment":"","msgTable":[[{"key":"count","value":"858"},{"key":"count","value":"200"},{"key":"count","value":"4.4875"},{"key":"count","value":"Godfather, The (1972)"}],[{"key":"average","value":"318"},{"key":"average","value":"311"},{"key":"average","value":"4.487138263665595"},{"key":"average","value":"Shawshank Redemption, The (1994)"}],[{"key":"title","value":"50"},{"key":"title","value":"201"},{"key":"title","value":"4.370646766169155"},{"key":"title","value":"Usual Suspects, The (1995)"}],[{"value":"527"},{"value":"244"},{"value":"4.30327868852459"},{"value":"Schindler's List (1993)"}],[{"value":"608"},{"value":"224"},{"value":"4.256696428571429"},{"value":"Fargo (1996)"}],[{"value":"296"},{"value":"324"},{"value":"4.256172839506172"},{"value":"Pulp Fiction (1994)"}],[{"value":"2858"},{"value":"220"},{"value":"4.236363636363636"},{"value":"American Beauty (1999)"}],[{"value":"1196"},{"value":"234"},{"value":"4.232905982905983"},{"value":"Star Wars: Episode V - The Empire Strikes Back (1980)"}],[{"value":"260"},{"value":"291"},{"value":"4.221649484536083"},{"value":"Star Wars: Episode IV - A New Hope (1977)"}],[{"value":"1198"},{"value":"220"},{"value":"4.193181818181818"},{"value":"Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)"}],[{"value":"2571"},{"value":"259"},{"value":"4.183397683397684"},{"value":"Matrix, The (1999)"}],[{"value":"4993"},{"value":"200"},{"value":"4.1825"},{"value":"Lord of the Rings: The Fellowship of the Ring, The (2001)"}],[{"value":"2959"},{"value":"202"},{"value":"4.178217821782178"},{"value":"Fight Club (1999)"}],[{"value":"593"},{"value":"304"},{"value":"4.1381578947368425"},{"value":"Silence of the Lambs, The (1991)"}],[{"value":"1210"},{"value":"217"},{"value":"4.059907834101383"},{"value":"Star Wars: Episode VI - Return of the Jedi (1983)"}],[{"value":"356"},{"value":"341"},{"value":"4.05425219941349"},{"value":"Forrest Gump (1994)"}],[{"value":"47"},{"value":"201"},{"value":"4.034825870646766"},{"value":"Seven (a.k.a. Se7en) (1995)"}],[{"value":"1270"},{"value":"226"},{"value":"4.015486725663717"},{"value":"Back to the Future (1985)"}],[{"value":"589"},{"value":"237"},{"value":"4.006329113924051"},{"value":"Terminator 2: Judgment Day (1991)"}],[{"value":"457"},{"value":"213"},{"value":"3.9530516431924885"},{"value":"Fugitive, The (1993)"}],[{"value":"110"},{"value":"228"},{"value":"3.9451754385964914"},{"value":"Braveheart (1995)"}],[{"value":"150"},{"value":"200"},{"value":"3.9025"},{"value":"Apollo 13 (1995)"}],[{"value":"1"},{"value":"247"},{"value":"3.8724696356275303"},{"value":"Toy Story (1995)"}],[{"value":"364"},{"value":"200"},{"value":"3.7775"},{"value":"Lion King, The (1994)"}],[{"value":"590"},{"value":"202"},{"value":"3.717821782178218"},{"value":"Dances with Wolves (1990)"}],[{"value":"480"},{"value":"274"},{"value":"3.7062043795620436"},{"value":"Jurassic Park (1993)"}],[{"value":"588"},{"value":"215"},{"value":"3.6744186046511627"},{"value":"Aladdin (1992)"}],[{"value":"780"},{"value":"218"},{"value":"3.4839449541284404"},{"value":"Independence Day (a.k.a. ID4) (1996)"}]],"columnNames":[{"name":"movieId","index":0,"aggr":"sum"},{"name":"count","index":1,"aggr":"sum"},{"name":"average","index":2,"aggr":"sum"},{"name":"title","index":3,"aggr":"sum"}],"rows":[["858","200","4.4875","Godfather, The (1972)"],["318","311","4.487138263665595","Shawshank Redemption, The (1994)"],["50","201","4.370646766169155","Usual Suspects, The (1995)"],["527","244","4.30327868852459","Schindler's List (1993)"],["608","224","4.256696428571429","Fargo (1996)"],["296","324","4.256172839506172","Pulp Fiction (1994)"],["2858","220","4.236363636363636","American Beauty (1999)"],["1196","234","4.232905982905983","Star Wars: Episode V - The Empire Strikes Back (1980)"],["260","291","4.221649484536083","Star Wars: Episode IV - A New Hope (1977)"],["1198","220","4.193181818181818","Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)"],["2571","259","4.183397683397684","Matrix, The (1999)"],["4993","200","4.1825","Lord of the Rings: The Fellowship of the Ring, The (2001)"],["2959","202","4.178217821782178","Fight Club (1999)"],["593","304","4.1381578947368425","Silence of the Lambs, The (1991)"],["1210","217","4.059907834101383","Star Wars: Episode VI - Return of the Jedi (1983)"],["356","341","4.05425219941349","Forrest Gump (1994)"],["47","201","4.034825870646766","Seven (a.k.a. Se7en) (1995)"],["1270","226","4.015486725663717","Back to the Future (1985)"],["589","237","4.006329113924051","Terminator 2: Judgment Day (1991)"],["457","213","3.9530516431924885","Fugitive, The (1993)"],["110","228","3.9451754385964914","Braveheart (1995)"],["150","200","3.9025","Apollo 13 (1995)"],["1","247","3.8724696356275303","Toy Story (1995)"],["364","200","3.7775","Lion King, The (1994)"],["590","202","3.717821782178218","Dances with Wolves (1990)"],["480","274","3.7062043795620436","Jurassic Park (1993)"],["588","215","3.6744186046511627","Aladdin (1992)"],["780","218","3.4839449541284404","Independence Day (a.k.a. ID4) (1996)"]]},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4696"},{"text":"%md\n\n### Task 5: Fill in your own ratings","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170221-202546_125265271","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Task 5: Fill in your own ratings</h3>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4697"},{"text":"%pyspark\n\nfrom pyspark.sql import Row\n\n# My user ID will be 0.\nmy_user_id = 0\n\n# MODIFY HERE\n# Format is:\n# (userid, movieID, rating)\nmy_rated_movies = [(0, 260, 4.4), \n                    (0, 904, 5.0), \n                    (0, 7502, 4.9), \n                    (0, 527, 4.8), \n                    (0, 77658, 4.4),\n                   (0, 4426, 3.9), \n                   (0, 2571, 5.0), \n                   (0, 593, 4.9), \n                   (0, 1136, 4.5), \n                   (0, 912, 4.0)]\n \n# For example, to give the movie \"Star Wars: Episode IV - A New Hope (1977)\" a five rating, you would add the following line:\n# (my_user_id, 260, 5)\n\n# Create data frame.\nmy_ratings_df = spark.createDataFrame(data = my_rated_movies, schema = ratings_df_schema)\n\nz.show(my_ratings_df.limit(10))\n","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"userId","index":0,"aggr":"sum"}],"values":[{"name":"movieId","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"userId","index":0,"aggr":"sum"},"yAxis":{"name":"movieId","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170119-115945_347312139","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4698","focus":true},{"text":"%md\n\n### Add own data to training data","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709039_-180695099","id":"20170221-203324_2147040918","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Add own data to training data</h3>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4699"},{"text":"%pyspark\n\ntraining_with_my_ratings_df = training_df.unionAll(my_ratings_df)","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709040_-170306879","id":"20170119-121947_1929464342","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4700"},{"text":"%md\n\n### Build a new model with our own data added","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709040_-170306879","id":"20170221-203355_1037615508","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Build a new model with our own data added</h3>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4701"},{"text":"%pyspark\n\nals.setPredictionCol(\"prediction\")\\\n   .setMaxIter(5)\\\n   .setSeed(seed)\\\n   .setRegParam(0.1)\\\n   .setUserCol('userId')\\\n   .setItemCol('movieId')\\\n   .setRatingCol('rating')\\\n   .setRank(12)\n\n# Create the model with these parameters.\nmy_ratings_model = als.fit(training_with_my_ratings_df)","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709040_-170306879","id":"20170221-202747_781594998","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4702"},{"text":"%md\n\n### Assess RMSE on new model","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709040_-170306879","id":"20170221-203545_961125665","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Assess RMSE on new model</h3>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4703"},{"text":"%pyspark\n\nmy_predict_df = my_ratings_model.transform(test_df)\n\n# Remove NaN values from prediction (due to SPARK-14489)\npredicted_test_my_ratings_df = my_predict_df.filter(my_predict_df.prediction != float('nan'))\n\n# Run the previously created RMSE evaluator, reg_eval, on the predicted_test_my_ratings_df DataFrame\ntest_RMSE_my_ratings = reg_eval.evaluate(predicted_test_my_ratings_df)\nprint('The model had a RMSE on the test set of {0}'.format(test_RMSE_my_ratings))","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709040_-170306879","id":"20170221-202811_1124537554","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4704","focus":true},{"text":"%md\n\n### Predict my un-rated movies","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709040_-170306879","id":"20170221-203607_54507434","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Predict my un-rated movies</h3>\n"},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4705"},{"text":"%pyspark\n\n# Create a list of my rated movie IDs\nmy_rated_movie_ids = [x[1] for x in my_rated_movies]\n\n# Filter out the movies I already rated.\nnot_rated_df = movies_df.filter(~ movies_df[\"ID\"].isin(my_rated_movie_ids))\n\n# Rename the \"ID\" column to be \"movieId\", and add a column with my_user_id as \"userId\".\n\nmy_unrated_movies_df = not_rated_df.withColumnRenamed('ID', 'movieId').withColumn('userId', F.lit(0))\n\n# Use my_rating_model to predict ratings for the movies that I did not manually rate.\nraw_predicted_ratings_df = my_ratings_model.transform(my_unrated_movies_df)\n\npredicted_ratings_df = raw_predicted_ratings_df.filter(raw_predicted_ratings_df['prediction'] != float('nan'))\n\nz.show(predicted_ratings_df.orderBy('prediction', ascending = False).limit(20))","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"movieId","index":0,"aggr":"sum"}],"values":[{"name":"title","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"movieId","index":0,"aggr":"sum"},"yAxis":{"name":"title","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709040_-170306879","id":"20170221-202833_748805080","dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4706","focus":true},{"text":"","dateUpdated":"2017-02-21T20:58:29+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487710709040_-170306879","id":"20170221-202904_587221855","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-21T20:58:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4707"}],"name":"AIA Movie Recommendation using ALS","id":"2CBVFDSNX","angularObjects":{"2CBMZ5NHR:shared_process":[],"2C9XETYF1:shared_process":[],"2C9ATXPC4:shared_process":[],"2C8SP5E4E:shared_process":[],"2C9REM5RK:shared_process":[],"2CC7ZG5M1:shared_process":[],"2CB4MG4PB:shared_process":[],"2C9FTZKZE:shared_process":[],"2CBDRFCWY:shared_process":[],"2CAY42D8Y:shared_process":[],"2C9H3ZY7P:shared_process":[],"2C9R4X19P:shared_process":[],"2C9P8SWS2:shared_process":[],"2CB5A2D5B:shared_process":[],"2C9K288XM:shared_process":[],"2CAC19P4J:shared_process":[],"2CBYC3W3G:shared_process":[],"2CA4ANQAV:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}
